{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d48906",
   "metadata": {},
   "source": [
    "# 10. 단어 Level로 번역기 업그레이드하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea858fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169dd6d",
   "metadata": {},
   "source": [
    "## 데이터 프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1fe2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35845</th>\n",
       "      <td>I failed on purpose.</td>\n",
       "      <td>J'ai fait exprès d'échouer.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>You look beautiful to me.</td>\n",
       "      <td>Tu es belle à mes yeux.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164014</th>\n",
       "      <td>Tom is very difficult to get along with.</td>\n",
       "      <td>Il est très difficile de s'entendre avec Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71588</th>\n",
       "      <td>I have certain standards.</td>\n",
       "      <td>J'ai certains principes.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169686</th>\n",
       "      <td>There's a bus stop in front of our school.</td>\n",
       "      <td>Il y a un arrêt de bus devant notre école.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               eng  \\\n",
       "35845                         I failed on purpose.   \n",
       "76694                    You look beautiful to me.   \n",
       "164014    Tom is very difficult to get along with.   \n",
       "71588                    I have certain standards.   \n",
       "169686  There's a bus stop in front of our school.   \n",
       "\n",
       "                                                  fra  \\\n",
       "35845                     J'ai fait exprès d'échouer.   \n",
       "76694                         Tu es belle à mes yeux.   \n",
       "164014  Il est très difficile de s'entendre avec Tom.   \n",
       "71588                        J'ai certains principes.   \n",
       "169686     Il y a un arrêt de bus devant notre école.   \n",
       "\n",
       "                                                       cc  \n",
       "35845   CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "76694   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "164014  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "71588   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "169686  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a678859",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f30563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27776</th>\n",
       "      <td>You can't beat me.</td>\n",
       "      <td>Tu ne peux pas me battre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Be calm.</td>\n",
       "      <td>Soyez calmes !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>Ignore them.</td>\n",
       "      <td>Ignorez-les.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>My hair's wet.</td>\n",
       "      <td>J'ai les cheveux mouillés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16392</th>\n",
       "      <td>Tom seemed busy.</td>\n",
       "      <td>Tom avait l'air occupé.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                         fra\n",
       "27776  You can't beat me.   Tu ne peux pas me battre.\n",
       "159              Be calm.              Soyez calmes !\n",
       "3525         Ignore them.                Ignorez-les.\n",
       "8291       My hair's wet.  J'ai les cheveux mouillés.\n",
       "16392    Tom seemed busy.     Tom avait l'air occupé."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85193f03",
   "metadata": {},
   "source": [
    "## 정제화,정규화, 전처리\n",
    "\n",
    "- 구두점 분리\n",
    "- 소문자 변환\n",
    "- 띄어쓰기 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f97b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9137eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sentence_decoder(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' ' + sentence + ' '\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65de6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x : new_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : new_sentence_decoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd985e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15492     [people, like, tom, .]\n",
       "4363          [you, re, rich, .]\n",
       "21996    [who, ordered, that, ?]\n",
       "14563     [i, was, convicted, .]\n",
       "10999    [i, m, in, the, car, .]\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8512f216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27258    [, nous, nous, dirigeons, vers, le, nord, ., ]\n",
       "32412               [, celui, ci, est, plus, gros, ., ]\n",
       "25294           [, ne, laissez, personne, chapper, !, ]\n",
       "19560                 [, je, n, ai, jamais, essay, ., ]\n",
       "31819                       [, c, est, arriv, ici, ., ]\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e64f6",
   "metadata": {},
   "source": [
    "## 토크나이저로 텍스트 ㅡ> 숫자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa762da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines.eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cc7682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 1], [28, 1], [28, 1], [28, 1], [761, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70af7940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 74, 7, 1],\n",
       " [1, 364, 2, 1],\n",
       " [1, 27, 511, 7, 1],\n",
       " [1, 715, 7, 1],\n",
       " [1, 749, 7, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer()\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a33746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7453\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49a701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d7b9f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7453\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010e3d8",
   "metadata": {},
   "source": [
    "## Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebad9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = ''\n",
    "eos_token = ''\n",
    "\n",
    "encoder_input = input_text\n",
    "\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6e95a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74, 7], [364, 2], [27, 511, 7]]\n",
      "[[74, 7], [364, 2], [27, 511, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0960b8",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f200a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46caac",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecb55e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 17)\n",
      "(30000, 17)\n",
      "(3000, 8)\n",
      "(3000, 17)\n",
      "(3000, 17)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb96ff",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c16c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, 256, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabb9f1",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2e7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(dec_masking, initial_state = encoder_states)\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8ac23",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6389e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1195776     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1907968     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 256)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7453)   1915421     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,069,789\n",
      "Trainable params: 6,069,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e4808ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367bdee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.4240 - acc: 0.9468 - val_loss: 0.3652 - val_acc: 0.9560\n",
      "Epoch 2/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.3280 - acc: 0.9606 - val_loss: 0.3123 - val_acc: 0.9640\n",
      "Epoch 3/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2848 - acc: 0.9670 - val_loss: 0.2785 - val_acc: 0.9688\n",
      "Epoch 4/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2554 - acc: 0.9713 - val_loss: 0.2609 - val_acc: 0.9724\n",
      "Epoch 5/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2370 - acc: 0.9742 - val_loss: 0.2448 - val_acc: 0.9747\n",
      "Epoch 6/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2189 - acc: 0.9767 - val_loss: 0.2302 - val_acc: 0.9766\n",
      "Epoch 7/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2031 - acc: 0.9786 - val_loss: 0.2187 - val_acc: 0.9783\n",
      "Epoch 8/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1904 - acc: 0.9800 - val_loss: 0.2101 - val_acc: 0.9794\n",
      "Epoch 9/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1806 - acc: 0.9812 - val_loss: 0.2021 - val_acc: 0.9803\n",
      "Epoch 10/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1729 - acc: 0.9821 - val_loss: 0.1956 - val_acc: 0.9811\n",
      "Epoch 11/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1663 - acc: 0.9828 - val_loss: 0.1906 - val_acc: 0.9816\n",
      "Epoch 12/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1618 - acc: 0.9834 - val_loss: 0.1868 - val_acc: 0.9819\n",
      "Epoch 13/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1575 - acc: 0.9838 - val_loss: 0.1838 - val_acc: 0.9822\n",
      "Epoch 14/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1529 - acc: 0.9842 - val_loss: 0.1795 - val_acc: 0.9827\n",
      "Epoch 15/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1488 - acc: 0.9846 - val_loss: 0.1772 - val_acc: 0.9829\n",
      "Epoch 16/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1457 - acc: 0.9849 - val_loss: 0.1740 - val_acc: 0.9829\n",
      "Epoch 17/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1427 - acc: 0.9851 - val_loss: 0.1697 - val_acc: 0.9831\n",
      "Epoch 18/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1382 - acc: 0.9854 - val_loss: 0.1648 - val_acc: 0.9833\n",
      "Epoch 19/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1328 - acc: 0.9856 - val_loss: 0.1603 - val_acc: 0.9832\n",
      "Epoch 20/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1278 - acc: 0.9858 - val_loss: 0.1552 - val_acc: 0.9836\n",
      "Epoch 21/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1234 - acc: 0.9862 - val_loss: 0.1511 - val_acc: 0.9839\n",
      "Epoch 22/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1199 - acc: 0.9864 - val_loss: 0.1486 - val_acc: 0.9840\n",
      "Epoch 23/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1171 - acc: 0.9866 - val_loss: 0.1462 - val_acc: 0.9842\n",
      "Epoch 24/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1147 - acc: 0.9869 - val_loss: 0.1445 - val_acc: 0.9845\n",
      "Epoch 25/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1128 - acc: 0.9872 - val_loss: 0.1421 - val_acc: 0.9848\n",
      "Epoch 26/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1106 - acc: 0.9876 - val_loss: 0.1404 - val_acc: 0.9849\n",
      "Epoch 27/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1085 - acc: 0.9878 - val_loss: 0.1393 - val_acc: 0.9851\n",
      "Epoch 28/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1066 - acc: 0.9881 - val_loss: 0.1378 - val_acc: 0.9851\n",
      "Epoch 29/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1048 - acc: 0.9884 - val_loss: 0.1362 - val_acc: 0.9853\n",
      "Epoch 30/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1031 - acc: 0.9885 - val_loss: 0.1353 - val_acc: 0.9854\n",
      "Epoch 31/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1015 - acc: 0.9887 - val_loss: 0.1340 - val_acc: 0.9856\n",
      "Epoch 32/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0999 - acc: 0.9888 - val_loss: 0.1328 - val_acc: 0.9858\n",
      "Epoch 33/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0983 - acc: 0.9890 - val_loss: 0.1315 - val_acc: 0.9859\n",
      "Epoch 34/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0970 - acc: 0.9891 - val_loss: 0.1305 - val_acc: 0.9860\n",
      "Epoch 35/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0957 - acc: 0.9892 - val_loss: 0.1294 - val_acc: 0.9860\n",
      "Epoch 36/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0944 - acc: 0.9893 - val_loss: 0.1288 - val_acc: 0.9860\n",
      "Epoch 37/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0935 - acc: 0.9893 - val_loss: 0.1279 - val_acc: 0.9860\n",
      "Epoch 38/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0925 - acc: 0.9894 - val_loss: 0.1272 - val_acc: 0.9861\n",
      "Epoch 39/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0916 - acc: 0.9894 - val_loss: 0.1263 - val_acc: 0.9862\n",
      "Epoch 40/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0908 - acc: 0.9895 - val_loss: 0.1260 - val_acc: 0.9861\n",
      "Epoch 41/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0900 - acc: 0.9895 - val_loss: 0.1253 - val_acc: 0.9861\n",
      "Epoch 42/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9896 - val_loss: 0.1249 - val_acc: 0.9862\n",
      "Epoch 43/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0887 - acc: 0.9896 - val_loss: 0.1242 - val_acc: 0.9862\n",
      "Epoch 44/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0881 - acc: 0.9896 - val_loss: 0.1243 - val_acc: 0.9862\n",
      "Epoch 45/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0877 - acc: 0.9896 - val_loss: 0.1236 - val_acc: 0.9862\n",
      "Epoch 46/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0874 - acc: 0.9896 - val_loss: 0.1235 - val_acc: 0.9862\n",
      "Epoch 47/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0871 - acc: 0.9896 - val_loss: 0.1234 - val_acc: 0.9862\n",
      "Epoch 48/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9896 - val_loss: 0.1229 - val_acc: 0.9862\n",
      "Epoch 49/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9896 - val_loss: 0.1226 - val_acc: 0.9863\n",
      "Epoch 50/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0863 - acc: 0.9896 - val_loss: 0.1229 - val_acc: 0.9863\n",
      "Epoch 51/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9896 - val_loss: 0.1222 - val_acc: 0.9863\n",
      "Epoch 52/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9896 - val_loss: 0.1222 - val_acc: 0.9863\n",
      "Epoch 53/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9896 - val_loss: 0.1222 - val_acc: 0.9863\n",
      "Epoch 54/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0864 - acc: 0.9896 - val_loss: 0.1225 - val_acc: 0.9863\n",
      "Epoch 55/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9896 - val_loss: 0.1224 - val_acc: 0.9863\n",
      "Epoch 56/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9896 - val_loss: 0.1227 - val_acc: 0.9863\n",
      "Epoch 57/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9896 - val_loss: 0.1226 - val_acc: 0.9862\n",
      "Epoch 58/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9896 - val_loss: 0.1226 - val_acc: 0.9862\n",
      "Epoch 59/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0869 - acc: 0.9896 - val_loss: 0.1229 - val_acc: 0.9863\n",
      "Epoch 60/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.1234 - val_acc: 0.9862\n",
      "Epoch 61/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0871 - acc: 0.9896 - val_loss: 0.1234 - val_acc: 0.9862\n",
      "Epoch 62/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1235 - val_acc: 0.9862\n",
      "Epoch 63/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0874 - acc: 0.9896 - val_loss: 0.1236 - val_acc: 0.9863\n",
      "Epoch 64/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.1238 - val_acc: 0.9862\n",
      "Epoch 65/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.1239 - val_acc: 0.9862\n",
      "Epoch 66/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0877 - acc: 0.9896 - val_loss: 0.1238 - val_acc: 0.9862\n",
      "Epoch 67/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0878 - acc: 0.9896 - val_loss: 0.1241 - val_acc: 0.9862\n",
      "Epoch 68/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0880 - acc: 0.9896 - val_loss: 0.1243 - val_acc: 0.9862\n",
      "Epoch 69/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0881 - acc: 0.9896 - val_loss: 0.1247 - val_acc: 0.9862\n",
      "Epoch 70/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0882 - acc: 0.9896 - val_loss: 0.1246 - val_acc: 0.9862\n",
      "Epoch 71/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0882 - acc: 0.9896 - val_loss: 0.1248 - val_acc: 0.9862\n",
      "Epoch 72/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0884 - acc: 0.9896 - val_loss: 0.1250 - val_acc: 0.9862\n",
      "Epoch 73/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1251 - val_acc: 0.9862\n",
      "Epoch 74/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1250 - val_acc: 0.9862\n",
      "Epoch 75/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1255 - val_acc: 0.9862\n",
      "Epoch 76/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0888 - acc: 0.9896 - val_loss: 0.1255 - val_acc: 0.9862\n",
      "Epoch 77/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0890 - acc: 0.9896 - val_loss: 0.1259 - val_acc: 0.9862\n",
      "Epoch 78/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0889 - acc: 0.9896 - val_loss: 0.1258 - val_acc: 0.9861\n",
      "Epoch 79/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0891 - acc: 0.9896 - val_loss: 0.1261 - val_acc: 0.9862\n",
      "Epoch 80/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0891 - acc: 0.9896 - val_loss: 0.1264 - val_acc: 0.9862\n",
      "Epoch 81/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0891 - acc: 0.9896 - val_loss: 0.1264 - val_acc: 0.9862\n",
      "Epoch 82/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0892 - acc: 0.9896 - val_loss: 0.1266 - val_acc: 0.9862\n",
      "Epoch 83/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9896 - val_loss: 0.1267 - val_acc: 0.9862\n",
      "Epoch 84/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9896 - val_loss: 0.1271 - val_acc: 0.9862\n",
      "Epoch 85/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9896 - val_loss: 0.1271 - val_acc: 0.9862\n",
      "Epoch 86/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1272 - val_acc: 0.9861\n",
      "Epoch 87/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9896 - val_loss: 0.1274 - val_acc: 0.9861\n",
      "Epoch 88/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1279 - val_acc: 0.9861\n",
      "Epoch 89/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9896 - val_loss: 0.1282 - val_acc: 0.9861\n",
      "Epoch 90/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1283 - val_acc: 0.9861\n",
      "Epoch 91/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1283 - val_acc: 0.9861\n",
      "Epoch 92/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9895 - val_loss: 0.1284 - val_acc: 0.9861\n",
      "Epoch 93/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1287 - val_acc: 0.9861\n",
      "Epoch 94/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1290 - val_acc: 0.9860\n",
      "Epoch 95/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1287 - val_acc: 0.9860\n",
      "Epoch 96/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1290 - val_acc: 0.9861\n",
      "Epoch 97/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1290 - val_acc: 0.9861\n",
      "Epoch 98/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1291 - val_acc: 0.9861\n",
      "Epoch 99/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1303 - val_acc: 0.9860\n",
      "Epoch 100/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9896 - val_loss: 0.1304 - val_acc: 0.9860\n",
      "Epoch 101/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1294 - val_acc: 0.9859\n",
      "Epoch 102/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1302 - val_acc: 0.9859\n",
      "Epoch 103/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1307 - val_acc: 0.9859\n",
      "Epoch 104/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0898 - acc: 0.9895 - val_loss: 0.1308 - val_acc: 0.9859\n",
      "Epoch 105/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1314 - val_acc: 0.9859\n",
      "Epoch 106/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1304 - val_acc: 0.9859\n",
      "Epoch 107/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1305 - val_acc: 0.9859\n",
      "Epoch 108/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1312 - val_acc: 0.9859\n",
      "Epoch 109/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1307 - val_acc: 0.9859\n",
      "Epoch 110/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1315 - val_acc: 0.9858\n",
      "Epoch 111/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9896 - val_loss: 0.1314 - val_acc: 0.9858\n",
      "Epoch 112/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0898 - acc: 0.9895 - val_loss: 0.1319 - val_acc: 0.9858\n",
      "Epoch 113/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1314 - val_acc: 0.9859\n",
      "Epoch 114/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0897 - acc: 0.9895 - val_loss: 0.1310 - val_acc: 0.9859\n",
      "Epoch 115/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9895 - val_loss: 0.1317 - val_acc: 0.9859\n",
      "Epoch 116/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1326 - val_acc: 0.9859\n",
      "Epoch 117/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1320 - val_acc: 0.9859\n",
      "Epoch 118/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9896 - val_loss: 0.1329 - val_acc: 0.9858\n",
      "Epoch 119/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9895 - val_loss: 0.1321 - val_acc: 0.9859\n",
      "Epoch 120/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1319 - val_acc: 0.9859\n",
      "Epoch 121/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0895 - acc: 0.9895 - val_loss: 0.1334 - val_acc: 0.9859\n",
      "Epoch 122/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1326 - val_acc: 0.9858\n",
      "Epoch 123/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1334 - val_acc: 0.9858\n",
      "Epoch 124/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1332 - val_acc: 0.9858\n",
      "Epoch 125/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1334 - val_acc: 0.9859\n",
      "Epoch 126/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0892 - acc: 0.9895 - val_loss: 0.1347 - val_acc: 0.9858\n",
      "Epoch 127/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1345 - val_acc: 0.9857\n",
      "Epoch 128/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1338 - val_acc: 0.9858\n",
      "Epoch 129/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0894 - acc: 0.9895 - val_loss: 0.1340 - val_acc: 0.9858\n",
      "Epoch 130/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1350 - val_acc: 0.9858\n",
      "Epoch 131/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1346 - val_acc: 0.9857\n",
      "Epoch 132/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0892 - acc: 0.9895 - val_loss: 0.1350 - val_acc: 0.9857\n",
      "Epoch 133/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1352 - val_acc: 0.9857\n",
      "Epoch 134/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0892 - acc: 0.9895 - val_loss: 0.1351 - val_acc: 0.9857\n",
      "Epoch 135/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1356 - val_acc: 0.9857\n",
      "Epoch 136/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0893 - acc: 0.9895 - val_loss: 0.1355 - val_acc: 0.9857\n",
      "Epoch 137/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0891 - acc: 0.9895 - val_loss: 0.1356 - val_acc: 0.9857\n",
      "Epoch 138/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0891 - acc: 0.9895 - val_loss: 0.1354 - val_acc: 0.9857\n",
      "Epoch 139/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0890 - acc: 0.9896 - val_loss: 0.1372 - val_acc: 0.9856\n",
      "Epoch 140/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0890 - acc: 0.9895 - val_loss: 0.1360 - val_acc: 0.9856\n",
      "Epoch 141/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0890 - acc: 0.9896 - val_loss: 0.1364 - val_acc: 0.9857\n",
      "Epoch 142/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0889 - acc: 0.9896 - val_loss: 0.1363 - val_acc: 0.9857\n",
      "Epoch 143/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0890 - acc: 0.9896 - val_loss: 0.1374 - val_acc: 0.9856\n",
      "Epoch 144/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0889 - acc: 0.9896 - val_loss: 0.1381 - val_acc: 0.9856\n",
      "Epoch 145/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0889 - acc: 0.9896 - val_loss: 0.1389 - val_acc: 0.9856\n",
      "Epoch 146/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0888 - acc: 0.9895 - val_loss: 0.1384 - val_acc: 0.9856\n",
      "Epoch 147/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0888 - acc: 0.9896 - val_loss: 0.1381 - val_acc: 0.9855\n",
      "Epoch 148/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0887 - acc: 0.9896 - val_loss: 0.1385 - val_acc: 0.9855\n",
      "Epoch 149/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0886 - acc: 0.9896 - val_loss: 0.1383 - val_acc: 0.9856\n",
      "Epoch 150/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0888 - acc: 0.9895 - val_loss: 0.1393 - val_acc: 0.9857\n",
      "Epoch 151/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0887 - acc: 0.9896 - val_loss: 0.1382 - val_acc: 0.9856\n",
      "Epoch 152/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0886 - acc: 0.9896 - val_loss: 0.1397 - val_acc: 0.9856\n",
      "Epoch 153/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1413 - val_acc: 0.9855\n",
      "Epoch 154/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0887 - acc: 0.9896 - val_loss: 0.1408 - val_acc: 0.9856\n",
      "Epoch 155/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0886 - acc: 0.9896 - val_loss: 0.1391 - val_acc: 0.9856\n",
      "Epoch 156/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0883 - acc: 0.9896 - val_loss: 0.1416 - val_acc: 0.9856\n",
      "Epoch 157/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1418 - val_acc: 0.9857\n",
      "Epoch 158/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0885 - acc: 0.9896 - val_loss: 0.1407 - val_acc: 0.9856\n",
      "Epoch 159/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0884 - acc: 0.9896 - val_loss: 0.1429 - val_acc: 0.9857\n",
      "Epoch 160/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0882 - acc: 0.9896 - val_loss: 0.1425 - val_acc: 0.9857\n",
      "Epoch 161/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0883 - acc: 0.9896 - val_loss: 0.1428 - val_acc: 0.9856\n",
      "Epoch 162/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0882 - acc: 0.9896 - val_loss: 0.1434 - val_acc: 0.9856\n",
      "Epoch 163/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0881 - acc: 0.9896 - val_loss: 0.1432 - val_acc: 0.9857\n",
      "Epoch 164/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0882 - acc: 0.9896 - val_loss: 0.1435 - val_acc: 0.9856\n",
      "Epoch 165/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0881 - acc: 0.9896 - val_loss: 0.1431 - val_acc: 0.9856\n",
      "Epoch 166/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0880 - acc: 0.9896 - val_loss: 0.1439 - val_acc: 0.9856\n",
      "Epoch 167/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0880 - acc: 0.9896 - val_loss: 0.1444 - val_acc: 0.9856\n",
      "Epoch 168/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0879 - acc: 0.9896 - val_loss: 0.1474 - val_acc: 0.9855\n",
      "Epoch 169/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0879 - acc: 0.9896 - val_loss: 0.1482 - val_acc: 0.9855\n",
      "Epoch 170/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0879 - acc: 0.9896 - val_loss: 0.1496 - val_acc: 0.9855\n",
      "Epoch 171/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0878 - acc: 0.9896 - val_loss: 0.1497 - val_acc: 0.9855\n",
      "Epoch 172/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0880 - acc: 0.9896 - val_loss: 0.1473 - val_acc: 0.9855\n",
      "Epoch 173/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0879 - acc: 0.9896 - val_loss: 0.1509 - val_acc: 0.9855\n",
      "Epoch 174/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0878 - acc: 0.9896 - val_loss: 0.1476 - val_acc: 0.9855\n",
      "Epoch 175/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0878 - acc: 0.9896 - val_loss: 0.1513 - val_acc: 0.9855\n",
      "Epoch 176/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0877 - acc: 0.9896 - val_loss: 0.1508 - val_acc: 0.9854\n",
      "Epoch 177/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.1526 - val_acc: 0.9855\n",
      "Epoch 178/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.1516 - val_acc: 0.9854\n",
      "Epoch 179/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0876 - acc: 0.9896 - val_loss: 0.1526 - val_acc: 0.9853\n",
      "Epoch 180/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0874 - acc: 0.9896 - val_loss: 0.1502 - val_acc: 0.9855\n",
      "Epoch 181/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0874 - acc: 0.9896 - val_loss: 0.1528 - val_acc: 0.9854\n",
      "Epoch 182/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0875 - acc: 0.9896 - val_loss: 0.1549 - val_acc: 0.9854\n",
      "Epoch 183/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1593 - val_acc: 0.9855\n",
      "Epoch 184/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1545 - val_acc: 0.9855\n",
      "Epoch 185/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0872 - acc: 0.9896 - val_loss: 0.1533 - val_acc: 0.9855\n",
      "Epoch 186/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1537 - val_acc: 0.9855\n",
      "Epoch 187/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1568 - val_acc: 0.9854\n",
      "Epoch 188/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0873 - acc: 0.9896 - val_loss: 0.1594 - val_acc: 0.9855\n",
      "Epoch 189/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0872 - acc: 0.9896 - val_loss: 0.1592 - val_acc: 0.9854\n",
      "Epoch 190/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0872 - acc: 0.9896 - val_loss: 0.1610 - val_acc: 0.9854\n",
      "Epoch 191/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.1590 - val_acc: 0.9854\n",
      "Epoch 192/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0871 - acc: 0.9896 - val_loss: 0.1595 - val_acc: 0.9854\n",
      "Epoch 193/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0869 - acc: 0.9896 - val_loss: 0.1592 - val_acc: 0.9855\n",
      "Epoch 194/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.1596 - val_acc: 0.9854\n",
      "Epoch 195/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0871 - acc: 0.9896 - val_loss: 0.1642 - val_acc: 0.9854\n",
      "Epoch 196/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0869 - acc: 0.9896 - val_loss: 0.1644 - val_acc: 0.9854\n",
      "Epoch 197/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.1627 - val_acc: 0.9855\n",
      "Epoch 198/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.1661 - val_acc: 0.9854\n",
      "Epoch 199/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0869 - acc: 0.9896 - val_loss: 0.1683 - val_acc: 0.9854\n",
      "Epoch 200/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9896 - val_loss: 0.1669 - val_acc: 0.9856\n",
      "Epoch 201/300\n",
      "938/938 [==============================] - 15s 17ms/step - loss: 0.0869 - acc: 0.9896 - val_loss: 0.1680 - val_acc: 0.9855\n",
      "Epoch 202/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0868 - acc: 0.9896 - val_loss: 0.1653 - val_acc: 0.9855\n",
      "Epoch 203/300\n",
      "938/938 [==============================] - 15s 17ms/step - loss: 0.0868 - acc: 0.9897 - val_loss: 0.1694 - val_acc: 0.9854\n",
      "Epoch 204/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9897 - val_loss: 0.1676 - val_acc: 0.9853\n",
      "Epoch 205/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9896 - val_loss: 0.1658 - val_acc: 0.9854\n",
      "Epoch 206/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0867 - acc: 0.9897 - val_loss: 0.1663 - val_acc: 0.9854\n",
      "Epoch 207/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0866 - acc: 0.9896 - val_loss: 0.1755 - val_acc: 0.9855\n",
      "Epoch 208/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0864 - acc: 0.9896 - val_loss: 0.1704 - val_acc: 0.9854\n",
      "Epoch 209/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9897 - val_loss: 0.1705 - val_acc: 0.9854\n",
      "Epoch 210/300\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0864 - acc: 0.9897 - val_loss: 0.1648 - val_acc: 0.9854\n",
      "Epoch 211/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9897 - val_loss: 0.1707 - val_acc: 0.9854\n",
      "Epoch 212/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9896 - val_loss: 0.1697 - val_acc: 0.9854\n",
      "Epoch 213/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9897 - val_loss: 0.1801 - val_acc: 0.9853\n",
      "Epoch 214/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0863 - acc: 0.9896 - val_loss: 0.1795 - val_acc: 0.9854\n",
      "Epoch 215/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9897 - val_loss: 0.1702 - val_acc: 0.9855\n",
      "Epoch 216/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0861 - acc: 0.9897 - val_loss: 0.1698 - val_acc: 0.9855\n",
      "Epoch 217/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9897 - val_loss: 0.1782 - val_acc: 0.9855\n",
      "Epoch 218/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0862 - acc: 0.9897 - val_loss: 0.1759 - val_acc: 0.9855\n",
      "Epoch 219/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0860 - acc: 0.9897 - val_loss: 0.1718 - val_acc: 0.9854\n",
      "Epoch 220/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0860 - acc: 0.9897 - val_loss: 0.1780 - val_acc: 0.9853\n",
      "Epoch 221/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0860 - acc: 0.9897 - val_loss: 0.1739 - val_acc: 0.9854\n",
      "Epoch 222/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0860 - acc: 0.9897 - val_loss: 0.1861 - val_acc: 0.9854\n",
      "Epoch 223/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0859 - acc: 0.9897 - val_loss: 0.1756 - val_acc: 0.9855\n",
      "Epoch 224/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0859 - acc: 0.9897 - val_loss: 0.1779 - val_acc: 0.9855\n",
      "Epoch 225/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0858 - acc: 0.9897 - val_loss: 0.1734 - val_acc: 0.9854\n",
      "Epoch 226/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0858 - acc: 0.9897 - val_loss: 0.1808 - val_acc: 0.9854\n",
      "Epoch 227/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0857 - acc: 0.9897 - val_loss: 0.1845 - val_acc: 0.9854\n",
      "Epoch 228/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0856 - acc: 0.9897 - val_loss: 0.1815 - val_acc: 0.9854\n",
      "Epoch 229/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0856 - acc: 0.9897 - val_loss: 0.1824 - val_acc: 0.9854\n",
      "Epoch 230/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0857 - acc: 0.9897 - val_loss: 0.1846 - val_acc: 0.9854\n",
      "Epoch 231/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0857 - acc: 0.9897 - val_loss: 0.1938 - val_acc: 0.9855\n",
      "Epoch 232/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0856 - acc: 0.9897 - val_loss: 0.1835 - val_acc: 0.9853\n",
      "Epoch 233/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0856 - acc: 0.9897 - val_loss: 0.1835 - val_acc: 0.9855\n",
      "Epoch 234/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0855 - acc: 0.9897 - val_loss: 0.1824 - val_acc: 0.9854\n",
      "Epoch 235/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0855 - acc: 0.9897 - val_loss: 0.1853 - val_acc: 0.9855\n",
      "Epoch 236/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0854 - acc: 0.9897 - val_loss: 0.1850 - val_acc: 0.9854\n",
      "Epoch 237/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0852 - acc: 0.9897 - val_loss: 0.1964 - val_acc: 0.9854\n",
      "Epoch 238/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0854 - acc: 0.9897 - val_loss: 0.1926 - val_acc: 0.9855\n",
      "Epoch 239/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0853 - acc: 0.9898 - val_loss: 0.1911 - val_acc: 0.9853\n",
      "Epoch 240/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0852 - acc: 0.9898 - val_loss: 0.1893 - val_acc: 0.9854\n",
      "Epoch 241/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0851 - acc: 0.9898 - val_loss: 0.1820 - val_acc: 0.9853\n",
      "Epoch 242/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0850 - acc: 0.9898 - val_loss: 0.1869 - val_acc: 0.9853\n",
      "Epoch 243/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0851 - acc: 0.9898 - val_loss: 0.1857 - val_acc: 0.9854\n",
      "Epoch 244/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0849 - acc: 0.9898 - val_loss: 0.1933 - val_acc: 0.9854\n",
      "Epoch 245/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0848 - acc: 0.9898 - val_loss: 0.1917 - val_acc: 0.9854\n",
      "Epoch 246/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0848 - acc: 0.9898 - val_loss: 0.2003 - val_acc: 0.9853\n",
      "Epoch 247/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0847 - acc: 0.9898 - val_loss: 0.2000 - val_acc: 0.9853\n",
      "Epoch 248/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0847 - acc: 0.9898 - val_loss: 0.1951 - val_acc: 0.9853\n",
      "Epoch 249/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0846 - acc: 0.9898 - val_loss: 0.1921 - val_acc: 0.9853\n",
      "Epoch 250/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0846 - acc: 0.9898 - val_loss: 0.1927 - val_acc: 0.9853\n",
      "Epoch 251/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0846 - acc: 0.9898 - val_loss: 0.2001 - val_acc: 0.9852\n",
      "Epoch 252/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0844 - acc: 0.9898 - val_loss: 0.2104 - val_acc: 0.9852\n",
      "Epoch 253/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0843 - acc: 0.9898 - val_loss: 0.1881 - val_acc: 0.9852\n",
      "Epoch 254/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0844 - acc: 0.9898 - val_loss: 0.1951 - val_acc: 0.9852\n",
      "Epoch 255/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0842 - acc: 0.9898 - val_loss: 0.1990 - val_acc: 0.9854\n",
      "Epoch 256/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0844 - acc: 0.9899 - val_loss: 0.2026 - val_acc: 0.9853\n",
      "Epoch 257/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0843 - acc: 0.9898 - val_loss: 0.1922 - val_acc: 0.9854\n",
      "Epoch 258/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0843 - acc: 0.9898 - val_loss: 0.2022 - val_acc: 0.9853\n",
      "Epoch 259/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0841 - acc: 0.9899 - val_loss: 0.1940 - val_acc: 0.9853\n",
      "Epoch 260/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0840 - acc: 0.9899 - val_loss: 0.1957 - val_acc: 0.9854\n",
      "Epoch 261/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0842 - acc: 0.9899 - val_loss: 0.2014 - val_acc: 0.9852\n",
      "Epoch 262/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0840 - acc: 0.9899 - val_loss: 0.1932 - val_acc: 0.9853\n",
      "Epoch 263/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0840 - acc: 0.9899 - val_loss: 0.1937 - val_acc: 0.9851\n",
      "Epoch 264/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0838 - acc: 0.9899 - val_loss: 0.1984 - val_acc: 0.9852\n",
      "Epoch 265/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0837 - acc: 0.9899 - val_loss: 0.1947 - val_acc: 0.9853\n",
      "Epoch 266/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0836 - acc: 0.9899 - val_loss: 0.1976 - val_acc: 0.9854\n",
      "Epoch 267/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0837 - acc: 0.9899 - val_loss: 0.2146 - val_acc: 0.9852\n",
      "Epoch 268/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0836 - acc: 0.9899 - val_loss: 0.1945 - val_acc: 0.9853\n",
      "Epoch 269/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0837 - acc: 0.9899 - val_loss: 0.2067 - val_acc: 0.9851\n",
      "Epoch 270/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0835 - acc: 0.9899 - val_loss: 0.1996 - val_acc: 0.9851\n",
      "Epoch 271/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0834 - acc: 0.9899 - val_loss: 0.2039 - val_acc: 0.9854\n",
      "Epoch 272/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0833 - acc: 0.9899 - val_loss: 0.1989 - val_acc: 0.9851\n",
      "Epoch 273/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0834 - acc: 0.9900 - val_loss: 0.2001 - val_acc: 0.9854\n",
      "Epoch 274/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0832 - acc: 0.9900 - val_loss: 0.1989 - val_acc: 0.9854\n",
      "Epoch 275/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0830 - acc: 0.9900 - val_loss: 0.1951 - val_acc: 0.9854\n",
      "Epoch 276/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0831 - acc: 0.9900 - val_loss: 0.2004 - val_acc: 0.9852\n",
      "Epoch 277/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0831 - acc: 0.9900 - val_loss: 0.2001 - val_acc: 0.9853\n",
      "Epoch 278/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0828 - acc: 0.9900 - val_loss: 0.2030 - val_acc: 0.9853\n",
      "Epoch 279/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0831 - acc: 0.9900 - val_loss: 0.2015 - val_acc: 0.9852\n",
      "Epoch 280/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0831 - acc: 0.9900 - val_loss: 0.2044 - val_acc: 0.9852\n",
      "Epoch 281/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0827 - acc: 0.9900 - val_loss: 0.2016 - val_acc: 0.9851\n",
      "Epoch 282/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0829 - acc: 0.9900 - val_loss: 0.2057 - val_acc: 0.9851\n",
      "Epoch 283/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0827 - acc: 0.9900 - val_loss: 0.2114 - val_acc: 0.9853\n",
      "Epoch 284/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0826 - acc: 0.9900 - val_loss: 0.2089 - val_acc: 0.9852\n",
      "Epoch 285/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0823 - acc: 0.9900 - val_loss: 0.1968 - val_acc: 0.9851\n",
      "Epoch 286/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0825 - acc: 0.9900 - val_loss: 0.2072 - val_acc: 0.9852\n",
      "Epoch 287/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0824 - acc: 0.9900 - val_loss: 0.1982 - val_acc: 0.9852\n",
      "Epoch 288/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0824 - acc: 0.9900 - val_loss: 0.2072 - val_acc: 0.9852\n",
      "Epoch 289/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0823 - acc: 0.9900 - val_loss: 0.1957 - val_acc: 0.9852\n",
      "Epoch 290/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0822 - acc: 0.9901 - val_loss: 0.2019 - val_acc: 0.9853\n",
      "Epoch 291/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0821 - acc: 0.9901 - val_loss: 0.1999 - val_acc: 0.9853\n",
      "Epoch 292/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0822 - acc: 0.9901 - val_loss: 0.2021 - val_acc: 0.9853\n",
      "Epoch 293/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0823 - acc: 0.9901 - val_loss: 0.2067 - val_acc: 0.9852\n",
      "Epoch 294/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0821 - acc: 0.9901 - val_loss: 0.1974 - val_acc: 0.9854\n",
      "Epoch 295/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0820 - acc: 0.9901 - val_loss: 0.1997 - val_acc: 0.9854\n",
      "Epoch 296/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0820 - acc: 0.9901 - val_loss: 0.1984 - val_acc: 0.9853\n",
      "Epoch 297/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0817 - acc: 0.9901 - val_loss: 0.2068 - val_acc: 0.9852\n",
      "Epoch 298/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0817 - acc: 0.9902 - val_loss: 0.2008 - val_acc: 0.9852\n",
      "Epoch 299/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0816 - acc: 0.9901 - val_loss: 0.2013 - val_acc: 0.9852\n",
      "Epoch 300/300\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0819 - acc: 0.9902 - val_loss: 0.2009 - val_acc: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f970822f9d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_train, decoder_input_train],decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 32, epochs = 300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863cea5",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b85a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         1195776   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,721,088\n",
      "Trainable params: 1,721,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482bdfd",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a924635",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24a8b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    1907968     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7453)   1915421     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,348,701\n",
      "Trainable params: 4,348,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95b6f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b856d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efa962eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dde23d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['']) and i!=fra2idx['']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "952e795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: stop squabbling . \n",
      "정답 문장: arr tez de vous chamailler . \n",
      "번역기가 번역한 문장:  uvre t vot est es\n",
      "-----------------------------------\n",
      "입력 문장: it s your bedtime . \n",
      "정답 문장: c est l heure d aller te coucher . \n",
      "번역기가 번역한 문장:  vous est positio\n",
      "-----------------------------------\n",
      "입력 문장: he writes arabic . \n",
      "정답 문장: il crit arabe . \n",
      "번역기가 번역한 문장:  d hurl croire croir\n",
      "-----------------------------------\n",
      "입력 문장: they re fearless . \n",
      "정답 문장: elles sont intr pides . \n",
      "번역기가 번역한 문장:  vous der entr ent\n",
      "-----------------------------------\n",
      "입력 문장: you crack me up . \n",
      "정답 문장: tu me fais mourir de rire . \n",
      "번역기가 번역한 문장:  entr fortement enduran\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,100,301,777,2222]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5e268",
   "metadata": {},
   "source": [
    "- epoch를 300을 주고 돌려보았지만, 50번대보다 정확성이 낮아졌다. 욕심만 과했던것 같다. 그래서 결과도 좋지 않은 듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d9d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
